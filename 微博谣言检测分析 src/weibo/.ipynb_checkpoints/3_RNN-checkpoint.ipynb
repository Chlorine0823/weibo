{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- load preprocessed data ----------\n",
      "x: (4664, 60, 10) x_len: (4664,) y: (4664,)\n",
      "---------- load saved pretrained embedding weight ----------\n",
      "vocab_embed: (30002, 200)\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "import numpy as np\n",
    "from data_util import DATA\n",
    "\n",
    "# Hyper parameters\n",
    "vocab_size = 30000\n",
    "N = 30\n",
    "topk = 10\n",
    "\n",
    "data = DATA(vocab_size=vocab_size, N=N, topk=topk)\n",
    "x, x_len, y = data.load_data()\n",
    "pretrained_weight = data.load_embed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "# Hyper parameters\n",
    "batch_size = 32\n",
    "test_ratio = 0.2\n",
    "embed_size = 200\n",
    "hidden_size = 64\n",
    "\n",
    "# random seed\n",
    "seed = 2018\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# transform into torch\n",
    "# separate into train/test set\n",
    "x_torch = torch.from_numpy(x).type(torch.LongTensor)\n",
    "y_torch = torch.from_numpy(y).type(torch.LongTensor)\n",
    "x_len_torch = torch.from_numpy(x_len).type(torch.LongTensor)\n",
    "\n",
    "data_num = x_torch.shape[0]\n",
    "split = int(data_num*test_ratio)\n",
    "indices = list(range(data_num))\n",
    "\n",
    "np.random.seed(seed)\n",
    "test_idx = np.random.choice(indices, size=split, replace=False)\n",
    "#print(test_idx)\n",
    "train_idx = list(set(indices) - set(test_idx))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "torch_dataset = Data.TensorDataset(x_torch, y_torch, x_len_torch)\n",
    "train_loader = torch.utils.data.DataLoader(torch_dataset, \n",
    "                batch_size=batch_size, sampler=train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(torch_dataset, \n",
    "                batch_size=len(test_idx), sampler=test_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (embed): Embedding(30002, 200, padding_idx=0)\n",
      "  (lstm): LSTM(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import utils as nn_utils\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size+2, embed_size, padding_idx=0)\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        self.lstm = nn.LSTM(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,# rnn hidden unit\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x, length_list):     \n",
    "        x_embed = self.embed(x)\n",
    "        # 1 - maxpool\n",
    "        #x= F.max_pool2d(x_embed, kernel_size=(x_embed.size(2),1)).squeeze(2)\n",
    "        # 2 - average\n",
    "        x = torch.mean(x_embed, dim=2)\n",
    "        # settle the problem of variable sequence\n",
    "        _, idx_sort = torch.sort(length_list, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        length_list_sort = list(length_list[idx_sort])\n",
    "        x = x.index_select(0, Variable(idx_sort))\n",
    "        x_pack = nn_utils.rnn.pack_padded_sequence(x, length_list_sort, batch_first=True)\n",
    "        output, (h_n, h_c) = self.lstm(x_pack, None)   # None represents zero initial hidden state\n",
    "        h_n = h_n[0][idx_unsort]\n",
    "        r_out = self.out(h_n)\n",
    "        return r_out\n",
    "\n",
    "lstm = LSTM()\n",
    "print(lstm)\n",
    "#r_out = lstm(t1,t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRU(\n",
      "  (embed): Embedding(30002, 200, padding_idx=0)\n",
      "  (gru): GRU(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import utils as nn_utils\n",
    "\n",
    "class GRU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GRU, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size+2, embed_size, padding_idx=0)\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        self.gru = nn.GRU(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,# rnn hidden unit\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x, length_list):     \n",
    "        x_embed = self.embed(x)\n",
    "        # 1 - maxpool\n",
    "        #x= F.max_pool2d(x_embed, kernel_size=(x_embed.size(2),1)).squeeze(2)\n",
    "        # 2 - average\n",
    "        x = torch.mean(x_embed, dim=2)\n",
    "        # settle the problem of variable sequence\n",
    "        _, idx_sort = torch.sort(length_list, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        length_list_sort = list(length_list[idx_sort])\n",
    "        x = x.index_select(0, Variable(idx_sort))\n",
    "        x_pack = nn_utils.rnn.pack_padded_sequence(x, length_list_sort, batch_first=True)\n",
    "        output, h_n = self.gru(x_pack, None)   # None represents zero initial hidden state\n",
    "        h_n = h_n[0][idx_unsort]\n",
    "        r_out = self.out(h_n)\n",
    "        return r_out\n",
    "\n",
    "gru = GRU()\n",
    "print(gru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embed): Embedding(30002, 200, padding_idx=0)\n",
      "  (rnn): RNN(200, 64, batch_first=True)\n",
      "  (out): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from torch.nn import utils as nn_utils\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size+2, embed_size, padding_idx=0)\n",
    "        self.embed.weight.data.copy_(torch.from_numpy(pretrained_weight))\n",
    "        self.rnn = nn.RNN(         # if use nn.RNN(), it hardly learns\n",
    "            input_size=embed_size,\n",
    "            hidden_size=hidden_size,# rnn hidden unit\n",
    "            num_layers=1,           # number of rnn layer\n",
    "            batch_first=True,       # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "\n",
    "        self.out = nn.Linear(hidden_size, 2)\n",
    "\n",
    "    def forward(self, x, length_list):     \n",
    "        x_embed = self.embed(x)\n",
    "        # 1 - maxpool\n",
    "        #x= F.max_pool2d(x_embed, kernel_size=(x_embed.size(2),1)).squeeze(2)\n",
    "        # 2 - average\n",
    "        x = torch.mean(x_embed, dim=2)\n",
    "        # settle the problem of variable sequence\n",
    "        _, idx_sort = torch.sort(length_list, dim=0, descending=True)\n",
    "        _, idx_unsort = torch.sort(idx_sort, dim=0)\n",
    "        length_list_sort = list(length_list[idx_sort])\n",
    "        x = x.index_select(0, Variable(idx_sort))\n",
    "        x_pack = nn_utils.rnn.pack_padded_sequence(x, length_list_sort, batch_first=True)\n",
    "        output, h_n = self.rnn(x_pack, None)   # None represents zero initial hidden state\n",
    "        h_n = h_n[0][idx_unsort]\n",
    "        r_out = self.out(h_n)\n",
    "        return r_out\n",
    "\n",
    "rnn = RNN()\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "class Train:\n",
    "    \n",
    "    def __init__(self, saved_path=\"./save\"):\n",
    "        self.saved_path = saved_path\n",
    "    \n",
    "    def save_best_model(self, model, model_name, exist_max_acc, new_acc, prf):\n",
    "        model_file = os.path.join(self.saved_path, \"%s.pkl\" % model_name)\n",
    "        if new_acc > exist_max_acc:\n",
    "            state = {\"net\": model.state_dict(), \"accuracy\": new_acc, \"prf\": prf}\n",
    "            torch.save(state, model_file)\n",
    "            #print(\"upgrade best model\")\n",
    "            \n",
    "    def show_best_result(self, model_name):\n",
    "        model_file = os.path.join(self.saved_path, \"%s.pkl\" % model_name)\n",
    "        s = torch.load(model_file)\n",
    "        print(\"\\nBest Result\\nAccuracy: %.3f\" % s[\"accuracy\"])\n",
    "        print(s[\"prf\"])\n",
    "        \n",
    "    def start(self, model, model_name, EPOCH=5, lr=0.001):\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr)   # optimize all cnn parameters\n",
    "        loss_func = nn.CrossEntropyLoss()                         # the target label is not one-hotted\n",
    "        exist_max_acc = 0\n",
    "\n",
    "        # training and testing\n",
    "        for epoch in range(EPOCH):\n",
    "            for step, (b_x, b_y, b_len) in enumerate(train_loader):  # gives batch data\n",
    "                output = model(b_x, b_len)                        # rnn output\n",
    "                loss = loss_func(output, b_y)                   # cross entropy loss\n",
    "                optimizer.zero_grad()                           # clear gradients for this training step\n",
    "                loss.backward()                                 # backpropagation, compute gradients\n",
    "                optimizer.step()                                # apply gradients\n",
    "\n",
    "                if step % 25 == 0:\n",
    "                    for x_test, y_test, x_len_test in test_loader:\n",
    "                        test_output = model(x_test, x_len_test)\n",
    "                    y_test = np.array(y_test)\n",
    "                    pred_y = torch.max(test_output, 1)[1].data.numpy().squeeze()\n",
    "                    accuracy = float((pred_y == y_test).astype(int).sum()) / float(y_test.size)\n",
    "                    prf = classification_report(y_test, pred_y, target_names=[\"non-rumor\", \"rumor\"], digits=3)\n",
    "\n",
    "                    self.save_best_model(model, model_name, exist_max_acc, accuracy, prf)\n",
    "                    exist_max_acc = max(exist_max_acc, accuracy)\n",
    "                    print(\"pred positive:\", pred_y.sum(), \"real positive:\", y_test.sum(), y_test.shape)\n",
    "                    print('Epoch: ', epoch+1, '| Step: %5d' % step, \n",
    "                          '| train loss: %.4f' % loss.data.numpy(), '| test accuracy: %.3f' % accuracy)\n",
    "            \n",
    "        self.show_best_result(model_name)\n",
    "                                    \n",
    "train = Train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred positive: 608 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:     0 | train loss: 0.7062 | test accuracy: 0.356\n",
      "pred positive: 173 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:    25 | train loss: 0.6708 | test accuracy: 0.606\n",
      "pred positive: 427 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:    50 | train loss: 0.5827 | test accuracy: 0.716\n",
      "pred positive: 578 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:    75 | train loss: 0.4706 | test accuracy: 0.745\n",
      "pred positive: 395 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:   100 | train loss: 0.5730 | test accuracy: 0.714\n",
      "pred positive: 521 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:     0 | train loss: 0.4357 | test accuracy: 0.750\n",
      "pred positive: 536 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:    25 | train loss: 0.5049 | test accuracy: 0.768\n",
      "pred positive: 574 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:    50 | train loss: 0.4452 | test accuracy: 0.766\n",
      "pred positive: 573 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:    75 | train loss: 0.2750 | test accuracy: 0.776\n",
      "pred positive: 531 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:   100 | train loss: 0.3511 | test accuracy: 0.782\n",
      "pred positive: 603 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:     0 | train loss: 0.4888 | test accuracy: 0.765\n",
      "pred positive: 561 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:    25 | train loss: 0.5682 | test accuracy: 0.782\n",
      "pred positive: 540 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:    50 | train loss: 0.6966 | test accuracy: 0.800\n",
      "pred positive: 563 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:    75 | train loss: 0.3389 | test accuracy: 0.789\n",
      "pred positive: 502 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:   100 | train loss: 0.4267 | test accuracy: 0.807\n",
      "pred positive: 576 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:     0 | train loss: 0.3416 | test accuracy: 0.803\n",
      "pred positive: 449 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:    25 | train loss: 0.3473 | test accuracy: 0.838\n",
      "pred positive: 502 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:    50 | train loss: 0.2899 | test accuracy: 0.858\n",
      "pred positive: 377 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:    75 | train loss: 0.1742 | test accuracy: 0.825\n",
      "pred positive: 414 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:   100 | train loss: 0.1691 | test accuracy: 0.863\n",
      "pred positive: 414 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:     0 | train loss: 0.1610 | test accuracy: 0.869\n",
      "pred positive: 445 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:    25 | train loss: 0.0672 | test accuracy: 0.892\n",
      "pred positive: 518 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:    50 | train loss: 0.0444 | test accuracy: 0.888\n",
      "pred positive: 508 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:    75 | train loss: 0.1652 | test accuracy: 0.903\n",
      "pred positive: 531 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:   100 | train loss: 0.2536 | test accuracy: 0.898\n",
      "pred positive: 389 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:     0 | train loss: 0.0817 | test accuracy: 0.866\n",
      "pred positive: 543 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:    25 | train loss: 0.1353 | test accuracy: 0.892\n",
      "pred positive: 395 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:    50 | train loss: 0.0947 | test accuracy: 0.864\n",
      "pred positive: 497 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:    75 | train loss: 0.0618 | test accuracy: 0.920\n",
      "pred positive: 505 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:   100 | train loss: 0.0776 | test accuracy: 0.917\n",
      "\n",
      "Best Result\n",
      "Accuracy: 0.920\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  non-rumor      0.952     0.885     0.917       468\n",
      "      rumor      0.891     0.955     0.922       464\n",
      "\n",
      "avg / total      0.922     0.920     0.919       932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.start(model=lstm, model_name=\"lstm\", EPOCH=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lchen/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred positive: 0 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:     0 | train loss: 0.6900 | test accuracy: 0.502\n",
      "pred positive: 216 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:    25 | train loss: 0.6853 | test accuracy: 0.614\n",
      "pred positive: 407 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:    50 | train loss: 0.4770 | test accuracy: 0.724\n",
      "pred positive: 699 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:    75 | train loss: 0.5282 | test accuracy: 0.711\n",
      "pred positive: 433 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:   100 | train loss: 0.3952 | test accuracy: 0.744\n",
      "pred positive: 562 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:     0 | train loss: 0.3219 | test accuracy: 0.773\n",
      "pred positive: 623 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:    25 | train loss: 0.5397 | test accuracy: 0.761\n",
      "pred positive: 402 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:    50 | train loss: 0.3844 | test accuracy: 0.742\n",
      "pred positive: 584 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:    75 | train loss: 0.3432 | test accuracy: 0.788\n",
      "pred positive: 618 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:   100 | train loss: 0.5477 | test accuracy: 0.770\n",
      "pred positive: 550 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:     0 | train loss: 0.3993 | test accuracy: 0.794\n",
      "pred positive: 499 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:    25 | train loss: 0.4911 | test accuracy: 0.808\n",
      "pred positive: 598 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:    50 | train loss: 0.3883 | test accuracy: 0.798\n",
      "pred positive: 306 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:    75 | train loss: 0.3275 | test accuracy: 0.747\n",
      "pred positive: 519 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:   100 | train loss: 0.3048 | test accuracy: 0.844\n",
      "pred positive: 508 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:     0 | train loss: 0.2404 | test accuracy: 0.856\n",
      "pred positive: 564 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:    25 | train loss: 0.2322 | test accuracy: 0.852\n",
      "pred positive: 563 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:    50 | train loss: 0.1645 | test accuracy: 0.877\n",
      "pred positive: 587 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:    75 | train loss: 0.3450 | test accuracy: 0.862\n",
      "pred positive: 524 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:   100 | train loss: 0.1506 | test accuracy: 0.888\n",
      "pred positive: 509 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:     0 | train loss: 0.0983 | test accuracy: 0.902\n",
      "pred positive: 410 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:    25 | train loss: 0.0729 | test accuracy: 0.882\n",
      "pred positive: 506 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:    50 | train loss: 0.0831 | test accuracy: 0.903\n",
      "pred positive: 525 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:    75 | train loss: 0.0928 | test accuracy: 0.905\n",
      "pred positive: 513 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:   100 | train loss: 0.1936 | test accuracy: 0.909\n",
      "pred positive: 481 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:     0 | train loss: 0.0738 | test accuracy: 0.911\n",
      "pred positive: 451 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:    25 | train loss: 0.1119 | test accuracy: 0.909\n",
      "pred positive: 455 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:    50 | train loss: 0.1607 | test accuracy: 0.907\n",
      "pred positive: 431 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:    75 | train loss: 0.0326 | test accuracy: 0.900\n",
      "pred positive: 521 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:   100 | train loss: 0.1365 | test accuracy: 0.915\n",
      "\n",
      "Best Result\n",
      "Accuracy: 0.915\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  non-rumor      0.973     0.855     0.910       468\n",
      "      rumor      0.869     0.976     0.920       464\n",
      "\n",
      "avg / total      0.922     0.915     0.915       932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.start(model=gru, model_name=\"gru\", EPOCH=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred positive: 141 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:     0 | train loss: 0.6976 | test accuracy: 0.486\n",
      "pred positive: 169 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:    25 | train loss: 0.7115 | test accuracy: 0.591\n",
      "pred positive: 318 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:    50 | train loss: 0.5669 | test accuracy: 0.665\n",
      "pred positive: 498 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:    75 | train loss: 0.7550 | test accuracy: 0.710\n",
      "pred positive: 478 real positive: 464 (932,)\n",
      "Epoch:  1 | Step:   100 | train loss: 0.4870 | test accuracy: 0.721\n",
      "pred positive: 140 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:     0 | train loss: 0.7717 | test accuracy: 0.590\n",
      "pred positive: 518 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:    25 | train loss: 0.6592 | test accuracy: 0.620\n",
      "pred positive: 317 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:    50 | train loss: 0.5716 | test accuracy: 0.658\n",
      "pred positive: 454 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:    75 | train loss: 0.5960 | test accuracy: 0.700\n",
      "pred positive: 357 real positive: 464 (932,)\n",
      "Epoch:  2 | Step:   100 | train loss: 0.4851 | test accuracy: 0.698\n",
      "pred positive: 551 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:     0 | train loss: 0.4993 | test accuracy: 0.739\n",
      "pred positive: 484 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:    25 | train loss: 0.4767 | test accuracy: 0.734\n",
      "pred positive: 519 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:    50 | train loss: 0.3843 | test accuracy: 0.746\n",
      "pred positive: 541 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:    75 | train loss: 0.4667 | test accuracy: 0.754\n",
      "pred positive: 609 real positive: 464 (932,)\n",
      "Epoch:  3 | Step:   100 | train loss: 0.3766 | test accuracy: 0.756\n",
      "pred positive: 405 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:     0 | train loss: 0.5277 | test accuracy: 0.746\n",
      "pred positive: 487 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:    25 | train loss: 0.2826 | test accuracy: 0.769\n",
      "pred positive: 457 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:    50 | train loss: 0.4593 | test accuracy: 0.780\n",
      "pred positive: 487 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:    75 | train loss: 0.5979 | test accuracy: 0.821\n",
      "pred positive: 395 real positive: 464 (932,)\n",
      "Epoch:  4 | Step:   100 | train loss: 0.5508 | test accuracy: 0.737\n",
      "pred positive: 556 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:     0 | train loss: 0.3595 | test accuracy: 0.796\n",
      "pred positive: 433 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:    25 | train loss: 0.1997 | test accuracy: 0.799\n",
      "pred positive: 563 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:    50 | train loss: 0.2613 | test accuracy: 0.834\n",
      "pred positive: 579 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:    75 | train loss: 0.3309 | test accuracy: 0.834\n",
      "pred positive: 541 real positive: 464 (932,)\n",
      "Epoch:  5 | Step:   100 | train loss: 0.2990 | test accuracy: 0.840\n",
      "pred positive: 491 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:     0 | train loss: 0.1091 | test accuracy: 0.879\n",
      "pred positive: 508 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:    25 | train loss: 0.2790 | test accuracy: 0.848\n",
      "pred positive: 467 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:    50 | train loss: 0.2875 | test accuracy: 0.883\n",
      "pred positive: 468 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:    75 | train loss: 0.1107 | test accuracy: 0.880\n",
      "pred positive: 521 real positive: 464 (932,)\n",
      "Epoch:  6 | Step:   100 | train loss: 0.2043 | test accuracy: 0.881\n",
      "\n",
      "Best Result\n",
      "Accuracy: 0.883\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  non-rumor      0.886     0.880     0.883       468\n",
      "      rumor      0.880     0.886     0.883       464\n",
      "\n",
      "avg / total      0.883     0.883     0.883       932\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.start(model=rnn, model_name=\"rnn\", EPOCH=6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
